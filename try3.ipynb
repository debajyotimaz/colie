{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/debajyoti/colie/colenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import LongformerModel, LongformerForSequenceClassification, LongformerForMultipleChoice\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "logging.basicConfig(filename=f'./logs/train_{time.asctime().replace(\" \",\"_\")}.log', filemode='w', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create a logger object\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a stream handler to print log messages to the console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "torch.manual_seed(40)\n",
    "np.random.seed(40)\n",
    "random.seed(40)\n",
    "torch.cuda.manual_seed(40)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file\n",
    "train_csv_file = \"/data1/debajyoti/colie/train.csv\"\n",
    "val_csv_file = \"/data1/debajyoti/colie/valid.csv\"\n",
    "test_csv_file = \"/data1/debajyoti/colie/test.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "train_labels = pd.read_csv(train_csv_file)\n",
    "val_labels = pd.read_csv(val_csv_file)\n",
    "test_labels = pd.read_csv(test_csv_file)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.BOOK_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the train folder\n",
    "train_folder = \"/data1/debajyoti/colie/train/train/\"\n",
    "# Define the path to the validation folder\n",
    "val_folder = \"/data1/debajyoti/colie/valid/valid/\"\n",
    "# Define the path to the test folder\n",
    "test_folder = \"/data1/debajyoti/colie/test/test/\"\n",
    "\n",
    "\n",
    "\n",
    "def create_df(folder, label):\n",
    "    # Initialize empty lists to store the data\n",
    "    text_data = []\n",
    "    labels = []\n",
    "    for index in label.index:\n",
    "        # filename = df_labels.BOOK_id[index]\n",
    "        # print(filename)\n",
    "        # print(df_labels['BOOK_id'][index], df_labels['Epoch'][index])\n",
    "        file_name = label['BOOK_id'][index]  # Assuming 'File Name' is the column name for the file names in the CSV\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "\n",
    "        # Read the text from the file\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Append the text and label to the respective lists\n",
    "        text_data.append(text)\n",
    "        labels.append(label['Epoch'][index].strip())  # Assuming 'Label' is the column name for the labels in the CSV\n",
    "        # break\n",
    "    return text_data, labels\n",
    "\n",
    "train_data, train_label = create_df(train_folder, train_labels)\n",
    "val_data, val_label = create_df(val_folder, val_labels)\n",
    "test_data, test_label = create_df(test_folder, test_labels)\n",
    "\n",
    "# Create a dataframe from the lists\n",
    "train = pd.DataFrame({'text': train_data, 'label': train_label})\n",
    "val = pd.DataFrame({'text': val_data, 'label': val_label})\n",
    "test = pd.DataFrame({'text': test_data, 'label': test_label})\n",
    "print(train.head(), val.head(), test.head())\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {'Romanticism':0,\n",
    "            'Viktorian':1,\n",
    "            'Modernism':2,\n",
    "            'PostModernism':3,\n",
    "            'OurDays':4}\n",
    "train['label'] = train['label'].map(label_dic)\n",
    "val['label'] = val['label'].map(label_dic)\n",
    "test['label'] = test['label'].map(label_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of text\n",
    "def length (txt):\n",
    "    length = len(txt.split())\n",
    "    return length\n",
    "\n",
    "txt_length = train['text'].apply(lambda x: length(x))\n",
    "print(txt_length.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length= 512\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        # Initialize thetokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the text and label from the dataframe\n",
    "        text = self.df.iloc[index]['text']\n",
    "        label = self.df.iloc[index]['label']\n",
    "\n",
    "        # Tokenize the text and convert it to input IDs\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=False,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "\n",
    "        # Return the input IDs and label as PyTorch tensors\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'][0],\n",
    "            'attention_mask': inputs['attention_mask'][0],\n",
    "            # 'token_type_ids': inputs['token_type_ids'][0],\n",
    "            'label': torch.tensor(label, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "# datasetclass = CustomDataset(tokenizer, train)\n",
    "train_dataset = CustomDataset(tokenizer, train)\n",
    "val_dataset = CustomDataset(tokenizer, val)\n",
    "test_dataset = CustomDataset(tokenizer, test)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 8\n",
    "train_dataloader = tqdm(DataLoader(train_dataset, batch_size=batch_size, shuffle=True))\n",
    "val_dataloader = tqdm(DataLoader(val_dataset, batch_size=batch_size, shuffle=False))\n",
    "test_dataloader = tqdm(DataLoader(test_dataset, batch_size=batch_size, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained('roberta-large')\n",
    "        # self.xlnet.resize_token_embeddings(num_tokens)\n",
    "        # self.transformer_encoder = TransformerEncoder(TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads), num_layers=num_layers)\n",
    "        #self.transformer_decoder = TransformerDecoder(TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads), num_layers=num_layers)\n",
    "        #self.transformer = Transformer(nhead=16, num_encoder_layers=6, num_decoder_layers = 6)\n",
    "        self.decoder = nn.Linear(self.roberta.config.hidden_size, num_labels) \n",
    "        # self.fc1 = nn.Linear(num_tokens, 2)\n",
    "        # self.fc2 = nn.Linear(num_tokens, 2)\n",
    "        # self.fc3 = nn.Linear(num_tokens, 5)\n",
    "        # self.num_classes = num_classes\n",
    "        # self.classifiers = nn.ModuleList([nn.Linear(self.roberta.config.hidden_size, num_classes[i]) for i in range(len(num_classes))])\n",
    "        # self.classifiers = nn.ModuleList([nn.Linear(num_tokens, num_classes[i]) for i in range(len(num_classes))])\n",
    "        # self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):  # src = [bsz, seq_len]\n",
    "        roberta_output = self.roberta(input_ids=input_ids).pooler_output\n",
    "        # print(long_output.shape)\n",
    "        # roberta_outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # last_hidden_state = outputs.last_hidden_state # Shape: (batch_size, sequence_len, hidden_size)\n",
    "        # src_embedded = last_hidden_state\n",
    "        # src_embedded = self.roberta.embeddings(src) # Use RoBERTa model to embed source sequence output: [bsz, seq_len, features,i.e. hidden_dim] [20, 100, 768]\n",
    "        # print(\"shape of roberta embeddings:\", src_embedded.shape)\n",
    "        #tgt_embedded = self.roberta.embeddings(tgt) # Use RoBERTa model to embed target sequence\n",
    "        # src_embedded = src_embedded # output: [bsz, seq_len, features] \n",
    "        # src_embedded = torch.cat([t1,t2,t3, src_embedded],1)\n",
    "\n",
    "        # t1 = torch.cat(src_embedded.size(0) * [t1])\n",
    "        # t2 = torch.cat(src_embedded.size(0) * [t2])\n",
    "        # t3 = torch.cat(src_embedded.size(0) * [t3])\n",
    "        # t = torch.stack([t1,t2,t3], dim=1)\n",
    "        # task_embedded = torch.cat([t, src_embedded],1)  # output shape: [bsz, seq_len, features] [8, 203, 768]\n",
    "\n",
    "        # memory = self.transformer_encoder(src_embedded)  # output shape: [bsz, seq_len, features] [8, 203, 768]\n",
    "        # print(\"shape after transformer encoder layer:\", memory.shape)\n",
    "        #output = self.transformer_decoder(tgt_embedded, memory)\n",
    "        #print(\"shape after transformer decoder layer:\", output.shape)\n",
    "\n",
    "        output = self.decoder(roberta_output)  # output shape: [bsz, seq_len, vocab_size] [8, 203, 50k]\n",
    "        # print(\"shape after transformer decoder layer:\", output.shape, output.dtype)\n",
    "        # task1_output = self.fc1(output[:,0,:])\n",
    "        # task2_output = self.fc2(output[:,1,:])\n",
    "        # task3_output = self.fc3(output[:,2,:num_classes])\n",
    "        # ae_output = output[:,len(self.num_classes):,:]\n",
    "        # ae_output = output[:,:,:]\n",
    "        # print(\"shape after final linear layer:\", output.shape)\n",
    "        # task_logits = [classifier(pooled_output) for classifier in self.classifiers]\n",
    "        # task_logits = []\n",
    "\n",
    "        # pooled_outputs = [output[:,i,:] for i in range(len(self.num_classes))] # output shape : [bsz, 1, vocab_size]\n",
    "\n",
    "        # for classifier, pooled_output in zip(self.classifiers, pooled_outputs):\n",
    "        #     # pooled_output = self.tanh(pooled_output)\n",
    "        #     logits = classifier(pooled_output)\n",
    "        #     task_logits.append(logits)\n",
    "        \n",
    "        return output\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 5\n",
    "\n",
    "model = TransformerModel(num_labels).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 5\n",
    "learning_rate = 2e-5\n",
    "class_weights = torch.tensor([0.35, 0.03, 0.03, 0.25, 0.34])\n",
    "\n",
    "# Set optimizer and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(logit, targets):\n",
    "    \"\"\"\n",
    "    Calculate accuracy and macro F1 score for each class\n",
    "    \"\"\"\n",
    "    # pos = list(task_dict.keys()).index(task_name)\n",
    "    # mask = torch.arange(targets.shape[0]).to(device)\n",
    "    # task_idx = mask[targets[:,pos] != 99]\n",
    "    output = logit\n",
    "    true_label = targets\n",
    "    # print(\"shapes for label:\", output.shape, true_label.shape)\n",
    "    pred_label = torch.argmax(output, dim=1).flatten().tolist()\n",
    "    true_label = true_label.flatten().tolist()\n",
    "\n",
    "\n",
    "    return pred_label, true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_train_loss = []\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 1\n",
    "    start_time = time.time()\n",
    "    num_batches = len(train_dataset) // batch_size\n",
    "    for batch, i in enumerate(train_dataloader):\n",
    "        data, mask, targets = i.values()\n",
    "        data = data.to(device)\n",
    "        mask = mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # print(data.dtype)        \n",
    "        # print(data.shape)\n",
    "        # task_logits, ae_output = model(data)\n",
    "        output = model(data, mask)\n",
    "        # t1_out, t2_out, t3_out, auto_output = model(data, t1, t2, t3)\n",
    "        # loss = custom_loss(logits_task1, logits_task2, logits_task3, targets)\n",
    "        # print(\"shape:\", data.shape, targets.flatten().shape)\n",
    "        # print(\"datatype:\", data.dtype, targets.flatten().dtype)\n",
    "        loss = criterion(output, targets.flatten())\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            # ppl = np.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                    f'lr {lr:02.7f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                    f'loss {cur_loss:5.5f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # if batch == 100:\n",
    "        #     break\n",
    "    current_train_loss.append(cur_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    # src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        for batch, i in enumerate(val_dataloader):\n",
    "            data, mask, targets = i.values()\n",
    "            data = data.to(device)\n",
    "            mask = mask.to(device)\n",
    "            targets = targets.to(device)\n",
    "            seq_len = data.size(1)\n",
    "            # logits_task1, logits_task2, logits_task3, ae_output = model(data, mask)\n",
    "            # task_logits, ae_output = model(data)\n",
    "            # task_logits, ae_output = model(data, mask)\n",
    "            output = model(data, mask)\n",
    "            # t1_out, t2_out, t3_out, auto_output = model(data, t1, t2, t3)\n",
    "            # loss = custom_loss(logits_task1, logits_task2, logits_task3, targets)\n",
    "            # loss = custom_loss(logits_task1, logits_task2, logits_task3, ae_output, data, targets)\n",
    "            loss = criterion(output, targets.flatten())\n",
    "\n",
    "            total_loss += seq_len * loss.item()\n",
    "\n",
    "            #get the labels for classification report\n",
    "            pred_label, true_label = get_labels(output, targets)\n",
    "            predictions.extend(pred_label)\n",
    "            true_labels.extend(true_label)\n",
    "            # if batch == 100:\n",
    "            #     break\n",
    "\n",
    "    # Compute overall classification report\n",
    "    logging.info(f\"\\n Scores:\")\n",
    "    logging.info(f\"\\n {classification_report(true_labels, predictions)}\")\n",
    "    return total_loss / (len(val_dataset) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"#\"* 89)\n",
    "logging.info(f\"\\n DESCRIPTION-> \\n logic: longformer + linear_layer + loss_reweighting(100 batches), model: {tokenizer.name_or_path}, lr:{learning_rate}, max_seq_length: {max_length}\")\n",
    "logging.info('#' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "current_val_loss = []   # for plotting graph of val_loss\n",
    "epochs = 40\n",
    "early_stop_thresh = 3\n",
    "\n",
    "tempdir = '/data1/debajyoti/colie/.temp/'\n",
    "# best_model_params_path = os.path.join(tempdir, f\"best_model_params_{time.asctime().replace(' ','_')}.pt\")\n",
    "best_model_params_path = os.path.join(tempdir, f\"best_model_params.pt\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model)\n",
    "    current_val_loss.append(val_loss)\n",
    "    # val_ppl = np.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logging.info('-' * 89)\n",
    "    logging.info(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "        f'valid loss {val_loss:5.5f}')\n",
    "    logging.info('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        logging.info(\"Early stopped training at epoch %d\" % epoch)\n",
    "        break  # terminate the training loop\n",
    "\n",
    "    scheduler.step()\n",
    "model.load_state_dict(torch.load(best_model_params_path)) # load best model states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to accumulate predictions and true labels for each task\n",
    "task_predictions = {task_name: [] for task_name in task_dict.keys()}\n",
    "task_true_labels = {task_name: [] for task_name in task_dict.keys()}\n",
    "\n",
    "# Evaluate model on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_dataloader):\n",
    "        # Extract data, mask, and targets from batch\n",
    "        data, mask, targets = batch.values()\n",
    "\n",
    "        # Move data and targets to device\n",
    "        data = data.to(device)\n",
    "        mask = mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "        seq_len = data.size(1)\n",
    "        \n",
    "        # forward pass through the model\n",
    "        # t1_out, t2_out, t3_out = model(data, mask)\n",
    "        # logits_task1, logits_task2, logits_task3, ae_output = model(data, mask)\n",
    "        task_logits, ae_output = model(data, mask)\n",
    "\n",
    "        # Accumulate predictions and true labels for each task\n",
    "        # outputs = [logits_task1, logits_task2, logits_task3]\n",
    "        outputs = task_logits\n",
    "        for task_name, task_output in zip(task_dict.keys(), outputs):\n",
    "            pred_labels, true_labels = get_labels(task_name, task_output, targets)\n",
    "            task_predictions[task_name].extend(pred_labels)\n",
    "            task_true_labels[task_name].extend(true_labels)\n",
    "        \n",
    "        # if batch == 400:\n",
    "        #     break\n",
    "\n",
    "# Compute overall classification report for each task\n",
    "for task_name, true_labels in task_true_labels.items():\n",
    "    pred_labels = task_predictions[task_name]\n",
    "    logging.info(f\"Scores of {task_name}:\")\n",
    "    logging.info(f\"\\n{classification_report(true_labels, pred_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longformer.embeddings.word_embeddings.weight: False\n",
      "longformer.embeddings.token_type_embeddings.weight: False\n",
      "longformer.embeddings.LayerNorm.weight: False\n",
      "longformer.embeddings.LayerNorm.bias: False\n",
      "longformer.embeddings.position_embeddings.weight: False\n",
      "longformer.encoder.layer.0.attention.self.query.weight: False\n",
      "longformer.encoder.layer.0.attention.self.query.bias: False\n",
      "longformer.encoder.layer.0.attention.self.key.weight: False\n",
      "longformer.encoder.layer.0.attention.self.key.bias: False\n",
      "longformer.encoder.layer.0.attention.self.value.weight: False\n",
      "longformer.encoder.layer.0.attention.self.value.bias: False\n",
      "longformer.encoder.layer.0.attention.self.query_global.weight: False\n",
      "longformer.encoder.layer.0.attention.self.query_global.bias: False\n",
      "longformer.encoder.layer.0.attention.self.key_global.weight: False\n",
      "longformer.encoder.layer.0.attention.self.key_global.bias: False\n",
      "longformer.encoder.layer.0.attention.self.value_global.weight: False\n",
      "longformer.encoder.layer.0.attention.self.value_global.bias: False\n",
      "longformer.encoder.layer.0.attention.output.dense.weight: False\n",
      "longformer.encoder.layer.0.attention.output.dense.bias: False\n",
      "longformer.encoder.layer.0.attention.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.0.attention.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.0.intermediate.dense.weight: False\n",
      "longformer.encoder.layer.0.intermediate.dense.bias: False\n",
      "longformer.encoder.layer.0.output.dense.weight: False\n",
      "longformer.encoder.layer.0.output.dense.bias: False\n",
      "longformer.encoder.layer.0.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.0.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.1.attention.self.query.weight: False\n",
      "longformer.encoder.layer.1.attention.self.query.bias: False\n",
      "longformer.encoder.layer.1.attention.self.key.weight: False\n",
      "longformer.encoder.layer.1.attention.self.key.bias: False\n",
      "longformer.encoder.layer.1.attention.self.value.weight: False\n",
      "longformer.encoder.layer.1.attention.self.value.bias: False\n",
      "longformer.encoder.layer.1.attention.self.query_global.weight: False\n",
      "longformer.encoder.layer.1.attention.self.query_global.bias: False\n",
      "longformer.encoder.layer.1.attention.self.key_global.weight: False\n",
      "longformer.encoder.layer.1.attention.self.key_global.bias: False\n",
      "longformer.encoder.layer.1.attention.self.value_global.weight: False\n",
      "longformer.encoder.layer.1.attention.self.value_global.bias: False\n",
      "longformer.encoder.layer.1.attention.output.dense.weight: False\n",
      "longformer.encoder.layer.1.attention.output.dense.bias: False\n",
      "longformer.encoder.layer.1.attention.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.1.attention.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.1.intermediate.dense.weight: False\n",
      "longformer.encoder.layer.1.intermediate.dense.bias: False\n",
      "longformer.encoder.layer.1.output.dense.weight: False\n",
      "longformer.encoder.layer.1.output.dense.bias: False\n",
      "longformer.encoder.layer.1.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.1.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.2.attention.self.query.weight: False\n",
      "longformer.encoder.layer.2.attention.self.query.bias: False\n",
      "longformer.encoder.layer.2.attention.self.key.weight: False\n",
      "longformer.encoder.layer.2.attention.self.key.bias: False\n",
      "longformer.encoder.layer.2.attention.self.value.weight: False\n",
      "longformer.encoder.layer.2.attention.self.value.bias: False\n",
      "longformer.encoder.layer.2.attention.self.query_global.weight: False\n",
      "longformer.encoder.layer.2.attention.self.query_global.bias: False\n",
      "longformer.encoder.layer.2.attention.self.key_global.weight: False\n",
      "longformer.encoder.layer.2.attention.self.key_global.bias: False\n",
      "longformer.encoder.layer.2.attention.self.value_global.weight: False\n",
      "longformer.encoder.layer.2.attention.self.value_global.bias: False\n",
      "longformer.encoder.layer.2.attention.output.dense.weight: False\n",
      "longformer.encoder.layer.2.attention.output.dense.bias: False\n",
      "longformer.encoder.layer.2.attention.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.2.attention.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.2.intermediate.dense.weight: False\n",
      "longformer.encoder.layer.2.intermediate.dense.bias: False\n",
      "longformer.encoder.layer.2.output.dense.weight: False\n",
      "longformer.encoder.layer.2.output.dense.bias: False\n",
      "longformer.encoder.layer.2.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.2.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.3.attention.self.query.weight: False\n",
      "longformer.encoder.layer.3.attention.self.query.bias: False\n",
      "longformer.encoder.layer.3.attention.self.key.weight: False\n",
      "longformer.encoder.layer.3.attention.self.key.bias: False\n",
      "longformer.encoder.layer.3.attention.self.value.weight: False\n",
      "longformer.encoder.layer.3.attention.self.value.bias: False\n",
      "longformer.encoder.layer.3.attention.self.query_global.weight: False\n",
      "longformer.encoder.layer.3.attention.self.query_global.bias: False\n",
      "longformer.encoder.layer.3.attention.self.key_global.weight: False\n",
      "longformer.encoder.layer.3.attention.self.key_global.bias: False\n",
      "longformer.encoder.layer.3.attention.self.value_global.weight: False\n",
      "longformer.encoder.layer.3.attention.self.value_global.bias: False\n",
      "longformer.encoder.layer.3.attention.output.dense.weight: False\n",
      "longformer.encoder.layer.3.attention.output.dense.bias: False\n",
      "longformer.encoder.layer.3.attention.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.3.attention.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.3.intermediate.dense.weight: False\n",
      "longformer.encoder.layer.3.intermediate.dense.bias: False\n",
      "longformer.encoder.layer.3.output.dense.weight: False\n",
      "longformer.encoder.layer.3.output.dense.bias: False\n",
      "longformer.encoder.layer.3.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.3.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.4.attention.self.query.weight: False\n",
      "longformer.encoder.layer.4.attention.self.query.bias: False\n",
      "longformer.encoder.layer.4.attention.self.key.weight: False\n",
      "longformer.encoder.layer.4.attention.self.key.bias: False\n",
      "longformer.encoder.layer.4.attention.self.value.weight: False\n",
      "longformer.encoder.layer.4.attention.self.value.bias: False\n",
      "longformer.encoder.layer.4.attention.self.query_global.weight: False\n",
      "longformer.encoder.layer.4.attention.self.query_global.bias: False\n",
      "longformer.encoder.layer.4.attention.self.key_global.weight: False\n",
      "longformer.encoder.layer.4.attention.self.key_global.bias: False\n",
      "longformer.encoder.layer.4.attention.self.value_global.weight: False\n",
      "longformer.encoder.layer.4.attention.self.value_global.bias: False\n",
      "longformer.encoder.layer.4.attention.output.dense.weight: False\n",
      "longformer.encoder.layer.4.attention.output.dense.bias: False\n",
      "longformer.encoder.layer.4.attention.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.4.attention.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.4.intermediate.dense.weight: False\n",
      "longformer.encoder.layer.4.intermediate.dense.bias: False\n",
      "longformer.encoder.layer.4.output.dense.weight: False\n",
      "longformer.encoder.layer.4.output.dense.bias: False\n",
      "longformer.encoder.layer.4.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.4.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.5.attention.self.query.weight: False\n",
      "longformer.encoder.layer.5.attention.self.query.bias: False\n",
      "longformer.encoder.layer.5.attention.self.key.weight: False\n",
      "longformer.encoder.layer.5.attention.self.key.bias: False\n",
      "longformer.encoder.layer.5.attention.self.value.weight: False\n",
      "longformer.encoder.layer.5.attention.self.value.bias: False\n",
      "longformer.encoder.layer.5.attention.self.query_global.weight: False\n",
      "longformer.encoder.layer.5.attention.self.query_global.bias: False\n",
      "longformer.encoder.layer.5.attention.self.key_global.weight: False\n",
      "longformer.encoder.layer.5.attention.self.key_global.bias: False\n",
      "longformer.encoder.layer.5.attention.self.value_global.weight: False\n",
      "longformer.encoder.layer.5.attention.self.value_global.bias: False\n",
      "longformer.encoder.layer.5.attention.output.dense.weight: False\n",
      "longformer.encoder.layer.5.attention.output.dense.bias: False\n",
      "longformer.encoder.layer.5.attention.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.5.attention.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.5.intermediate.dense.weight: False\n",
      "longformer.encoder.layer.5.intermediate.dense.bias: False\n",
      "longformer.encoder.layer.5.output.dense.weight: False\n",
      "longformer.encoder.layer.5.output.dense.bias: False\n",
      "longformer.encoder.layer.5.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.5.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.6.attention.self.query.weight: False\n",
      "longformer.encoder.layer.6.attention.self.query.bias: False\n",
      "longformer.encoder.layer.6.attention.self.key.weight: False\n",
      "longformer.encoder.layer.6.attention.self.key.bias: False\n",
      "longformer.encoder.layer.6.attention.self.value.weight: False\n",
      "longformer.encoder.layer.6.attention.self.value.bias: False\n",
      "longformer.encoder.layer.6.attention.self.query_global.weight: False\n",
      "longformer.encoder.layer.6.attention.self.query_global.bias: False\n",
      "longformer.encoder.layer.6.attention.self.key_global.weight: False\n",
      "longformer.encoder.layer.6.attention.self.key_global.bias: False\n",
      "longformer.encoder.layer.6.attention.self.value_global.weight: False\n",
      "longformer.encoder.layer.6.attention.self.value_global.bias: False\n",
      "longformer.encoder.layer.6.attention.output.dense.weight: False\n",
      "longformer.encoder.layer.6.attention.output.dense.bias: False\n",
      "longformer.encoder.layer.6.attention.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.6.attention.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.6.intermediate.dense.weight: False\n",
      "longformer.encoder.layer.6.intermediate.dense.bias: False\n",
      "longformer.encoder.layer.6.output.dense.weight: False\n",
      "longformer.encoder.layer.6.output.dense.bias: False\n",
      "longformer.encoder.layer.6.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.6.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.7.attention.self.query.weight: False\n",
      "longformer.encoder.layer.7.attention.self.query.bias: False\n",
      "longformer.encoder.layer.7.attention.self.key.weight: False\n",
      "longformer.encoder.layer.7.attention.self.key.bias: False\n",
      "longformer.encoder.layer.7.attention.self.value.weight: False\n",
      "longformer.encoder.layer.7.attention.self.value.bias: False\n",
      "longformer.encoder.layer.7.attention.self.query_global.weight: False\n",
      "longformer.encoder.layer.7.attention.self.query_global.bias: False\n",
      "longformer.encoder.layer.7.attention.self.key_global.weight: False\n",
      "longformer.encoder.layer.7.attention.self.key_global.bias: False\n",
      "longformer.encoder.layer.7.attention.self.value_global.weight: False\n",
      "longformer.encoder.layer.7.attention.self.value_global.bias: False\n",
      "longformer.encoder.layer.7.attention.output.dense.weight: False\n",
      "longformer.encoder.layer.7.attention.output.dense.bias: False\n",
      "longformer.encoder.layer.7.attention.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.7.attention.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.7.intermediate.dense.weight: False\n",
      "longformer.encoder.layer.7.intermediate.dense.bias: False\n",
      "longformer.encoder.layer.7.output.dense.weight: False\n",
      "longformer.encoder.layer.7.output.dense.bias: False\n",
      "longformer.encoder.layer.7.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.7.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.8.attention.self.query.weight: False\n",
      "longformer.encoder.layer.8.attention.self.query.bias: False\n",
      "longformer.encoder.layer.8.attention.self.key.weight: False\n",
      "longformer.encoder.layer.8.attention.self.key.bias: False\n",
      "longformer.encoder.layer.8.attention.self.value.weight: False\n",
      "longformer.encoder.layer.8.attention.self.value.bias: False\n",
      "longformer.encoder.layer.8.attention.self.query_global.weight: False\n",
      "longformer.encoder.layer.8.attention.self.query_global.bias: False\n",
      "longformer.encoder.layer.8.attention.self.key_global.weight: False\n",
      "longformer.encoder.layer.8.attention.self.key_global.bias: False\n",
      "longformer.encoder.layer.8.attention.self.value_global.weight: False\n",
      "longformer.encoder.layer.8.attention.self.value_global.bias: False\n",
      "longformer.encoder.layer.8.attention.output.dense.weight: False\n",
      "longformer.encoder.layer.8.attention.output.dense.bias: False\n",
      "longformer.encoder.layer.8.attention.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.8.attention.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.8.intermediate.dense.weight: False\n",
      "longformer.encoder.layer.8.intermediate.dense.bias: False\n",
      "longformer.encoder.layer.8.output.dense.weight: False\n",
      "longformer.encoder.layer.8.output.dense.bias: False\n",
      "longformer.encoder.layer.8.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.8.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.9.attention.self.query.weight: False\n",
      "longformer.encoder.layer.9.attention.self.query.bias: False\n",
      "longformer.encoder.layer.9.attention.self.key.weight: False\n",
      "longformer.encoder.layer.9.attention.self.key.bias: False\n",
      "longformer.encoder.layer.9.attention.self.value.weight: False\n",
      "longformer.encoder.layer.9.attention.self.value.bias: False\n",
      "longformer.encoder.layer.9.attention.self.query_global.weight: False\n",
      "longformer.encoder.layer.9.attention.self.query_global.bias: False\n",
      "longformer.encoder.layer.9.attention.self.key_global.weight: False\n",
      "longformer.encoder.layer.9.attention.self.key_global.bias: False\n",
      "longformer.encoder.layer.9.attention.self.value_global.weight: False\n",
      "longformer.encoder.layer.9.attention.self.value_global.bias: False\n",
      "longformer.encoder.layer.9.attention.output.dense.weight: False\n",
      "longformer.encoder.layer.9.attention.output.dense.bias: False\n",
      "longformer.encoder.layer.9.attention.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.9.attention.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.9.intermediate.dense.weight: False\n",
      "longformer.encoder.layer.9.intermediate.dense.bias: False\n",
      "longformer.encoder.layer.9.output.dense.weight: False\n",
      "longformer.encoder.layer.9.output.dense.bias: False\n",
      "longformer.encoder.layer.9.output.LayerNorm.weight: False\n",
      "longformer.encoder.layer.9.output.LayerNorm.bias: False\n",
      "longformer.encoder.layer.10.attention.self.query.weight: True\n",
      "longformer.encoder.layer.10.attention.self.query.bias: True\n",
      "longformer.encoder.layer.10.attention.self.key.weight: True\n",
      "longformer.encoder.layer.10.attention.self.key.bias: True\n",
      "longformer.encoder.layer.10.attention.self.value.weight: True\n",
      "longformer.encoder.layer.10.attention.self.value.bias: True\n",
      "longformer.encoder.layer.10.attention.self.query_global.weight: True\n",
      "longformer.encoder.layer.10.attention.self.query_global.bias: True\n",
      "longformer.encoder.layer.10.attention.self.key_global.weight: True\n",
      "longformer.encoder.layer.10.attention.self.key_global.bias: True\n",
      "longformer.encoder.layer.10.attention.self.value_global.weight: True\n",
      "longformer.encoder.layer.10.attention.self.value_global.bias: True\n",
      "longformer.encoder.layer.10.attention.output.dense.weight: True\n",
      "longformer.encoder.layer.10.attention.output.dense.bias: True\n",
      "longformer.encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "longformer.encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "longformer.encoder.layer.10.intermediate.dense.weight: True\n",
      "longformer.encoder.layer.10.intermediate.dense.bias: True\n",
      "longformer.encoder.layer.10.output.dense.weight: True\n",
      "longformer.encoder.layer.10.output.dense.bias: True\n",
      "longformer.encoder.layer.10.output.LayerNorm.weight: True\n",
      "longformer.encoder.layer.10.output.LayerNorm.bias: True\n",
      "longformer.encoder.layer.11.attention.self.query.weight: True\n",
      "longformer.encoder.layer.11.attention.self.query.bias: True\n",
      "longformer.encoder.layer.11.attention.self.key.weight: True\n",
      "longformer.encoder.layer.11.attention.self.key.bias: True\n",
      "longformer.encoder.layer.11.attention.self.value.weight: True\n",
      "longformer.encoder.layer.11.attention.self.value.bias: True\n",
      "longformer.encoder.layer.11.attention.self.query_global.weight: True\n",
      "longformer.encoder.layer.11.attention.self.query_global.bias: True\n",
      "longformer.encoder.layer.11.attention.self.key_global.weight: True\n",
      "longformer.encoder.layer.11.attention.self.key_global.bias: True\n",
      "longformer.encoder.layer.11.attention.self.value_global.weight: True\n",
      "longformer.encoder.layer.11.attention.self.value_global.bias: True\n",
      "longformer.encoder.layer.11.attention.output.dense.weight: True\n",
      "longformer.encoder.layer.11.attention.output.dense.bias: True\n",
      "longformer.encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "longformer.encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "longformer.encoder.layer.11.intermediate.dense.weight: True\n",
      "longformer.encoder.layer.11.intermediate.dense.bias: True\n",
      "longformer.encoder.layer.11.output.dense.weight: True\n",
      "longformer.encoder.layer.11.output.dense.bias: True\n",
      "longformer.encoder.layer.11.output.LayerNorm.weight: True\n",
      "longformer.encoder.layer.11.output.LayerNorm.bias: True\n",
      "classifier.dense.weight: False\n",
      "classifier.dense.bias: False\n",
      "classifier.out_proj.weight: False\n",
      "classifier.out_proj.bias: False\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerForSequenceClassification\n",
    "\n",
    "# Load the Longformer model\n",
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=5)\n",
    "\n",
    "# # Freeze all but the last two layers\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'layer' in name:  # Check if the parameter belongs to a layer\n",
    "#         param.requires_grad = False  # Freeze the parameter\n",
    "# Freeze all layers except the top 2\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the parameters of the top 2 layers\n",
    "for param in model.longformer.encoder.layer[-2:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify the parameters' `requires_grad` status\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name}: {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
