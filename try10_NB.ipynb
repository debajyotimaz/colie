{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/debajyoti/colie/colenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import LongformerModel, AutoTokenizer, LongformerForSequenceClassification, LongformerForMultipleChoice\n",
    "from transformers import AutoTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "logging.basicConfig(filename=f'./logs/train_{time.asctime().replace(\" \",\"_\")}.log', filemode='w', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create a logger object\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a stream handler to print log messages to the console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "torch.manual_seed(40)\n",
    "np.random.seed(40)\n",
    "random.seed(40)\n",
    "torch.cuda.manual_seed(40)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOOK_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7616_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7616_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7616_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7616_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7616_5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143025</th>\n",
       "      <td>5677_92.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143026</th>\n",
       "      <td>5677_93.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143027</th>\n",
       "      <td>5677_94.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143028</th>\n",
       "      <td>5677_95.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143029</th>\n",
       "      <td>5677_96.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143030 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BOOK_id\n",
       "0        7616_1.txt\n",
       "1        7616_2.txt\n",
       "2        7616_3.txt\n",
       "3        7616_4.txt\n",
       "4        7616_5.txt\n",
       "...             ...\n",
       "143025  5677_92.txt\n",
       "143026  5677_93.txt\n",
       "143027  5677_94.txt\n",
       "143028  5677_95.txt\n",
       "143029  5677_96.txt\n",
       "\n",
       "[143030 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the CSV file\n",
    "train_csv_file = \"/data1/debajyoti/colie/train.csv\"\n",
    "val_csv_file = \"/data1/debajyoti/colie/valid.csv\"\n",
    "test_csv_file = \"/data1/debajyoti/colie/test.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "train_labels = pd.read_csv(train_csv_file)\n",
    "val_labels = pd.read_csv(val_csv_file)\n",
    "test_labels = pd.read_csv(test_csv_file)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'27993_1.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.BOOK_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text      label\n",
      "0  rifle; Ivan's was a double-barrelled shot-gun ...  Viktorian\n",
      "1  upon the track of the bear. After following it...  Viktorian\n",
      "2  to pull him out with their hands--even had the...  Viktorian\n",
      "3  a slight sparkle of scientific conceit, \"this ...  Viktorian\n",
      "4  bears with a white ring round their necks? Yes...  Viktorian                                                 text      label\n",
      "0  kind good morning, and returned her hearty emb...  Viktorian\n",
      "1  sky, and of the moon, which clothed the old pi...  Viktorian\n",
      "2  left Rome for Augsburg, my mind being much exc...  Viktorian\n",
      "3  thoughts some of the old melodies he knew by h...  Viktorian\n",
      "4  \"But,\" said Henry, \"is it not possible that th...  Viktorian                                                 text\n",
      "0  \"Alas, poor girl!\" said I, \"I fear that her ha...\n",
      "1  to divide her attention between the said garco...\n",
      "2  visitor's disposition to gallantry. However, s...\n",
      "3  says Juvenal, \"'Mors sola fatetur Quantula sin...\n",
      "4  him out in that back passage; the outer door i...\n",
      "(546210, 2) (36257, 2) (143030, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the train folder\n",
    "train_folder = \"/data1/debajyoti/colie/train/train/\"\n",
    "# Define the path to the validation folder\n",
    "val_folder = \"/data1/debajyoti/colie/valid/valid/\"\n",
    "# Define the path to the test folder\n",
    "test_folder = \"/data1/debajyoti/colie/test/test/\"\n",
    "\n",
    "\n",
    "\n",
    "def create_df(folder, label):\n",
    "    # Initialize empty lists to store the data\n",
    "    text_data = []\n",
    "    labels = []\n",
    "    for index in label.index:\n",
    "        # filename = df_labels.BOOK_id[index]\n",
    "        # print(filename)\n",
    "        # print(df_labels['BOOK_id'][index], df_labels['Epoch'][index])\n",
    "        file_name = label['BOOK_id'][index]  # Assuming 'File Name' is the column name for the file names in the CSV\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "\n",
    "        # Read the text from the file\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Append the text and label to the respective lists\n",
    "        text_data.append(text)\n",
    "        labels.append(label['Epoch'][index].strip())  # Assuming 'Label' is the column name for the labels in the CSV\n",
    "        # break\n",
    "    return text_data, labels\n",
    "\n",
    "def create_df_test(folder, label):\n",
    "    # Initialize empty lists to store the data\n",
    "    text_data = []\n",
    "    # labels = []\n",
    "    for index in label.index:\n",
    "        # filename = df_labels.BOOK_id[index]\n",
    "        # print(filename)\n",
    "        # print(df_labels['BOOK_id'][index], df_labels['Epoch'][index])\n",
    "        file_name = label['BOOK_id'][index]  # Assuming 'File Name' is the column name for the file names in the CSV\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "\n",
    "        # Read the text from the file\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Append the text and label to the respective lists\n",
    "        text_data.append(text)\n",
    "        # labels.append(label['Epoch'][index].strip())  # Assuming 'Label' is the column name for the labels in the CSV\n",
    "        # break\n",
    "    return text_data\n",
    "\n",
    "train_data, train_label = create_df(train_folder, train_labels)\n",
    "val_data, val_label = create_df(val_folder, val_labels)\n",
    "test_data = create_df_test(test_folder, test_labels)\n",
    "\n",
    "# Create a dataframe from the lists\n",
    "train = pd.DataFrame({'text': train_data, 'label': train_label})\n",
    "val = pd.DataFrame({'text': val_data, 'label': val_label})\n",
    "test = pd.DataFrame({'text': test_data})\n",
    "print(train.head(), val.head(), test.head())\n",
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {'Romanticism':0,\n",
    "            'Viktorian':1,\n",
    "            'Modernism':2,\n",
    "            'PostModernism':3,\n",
    "            'OurDays':4}\n",
    "train['label'] = train['label'].map(label_dic)\n",
    "val['label'] = val['label'].map(label_dic)\n",
    "# test['label'] = test['label'].map(label_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483268    1128\n",
      "483267    1068\n",
      "521384    1065\n",
      "483265    1034\n",
      "81542     1020\n",
      "          ... \n",
      "470405       1\n",
      "130188       1\n",
      "217335       1\n",
      "351867       1\n",
      "368135       1\n",
      "Name: text, Length: 546210, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Length of text\n",
    "def length (txt):\n",
    "    length = len(txt.split())\n",
    "    return length\n",
    "\n",
    "txt_length = train['text'].apply(lambda x: length(x))\n",
    "print(txt_length.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    16938\n",
       "2    14848\n",
       "3     1713\n",
       "4     1600\n",
       "0     1158\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2134 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "max_length= 500\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        # Initialize thetokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the text and label from the dataframe\n",
    "        text = self.df.iloc[index]['text']\n",
    "        label = self.df.iloc[index]['label']\n",
    "\n",
    "        # Tokenize the text and convert it to input IDs\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=False,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "\n",
    "        # Return the input IDs and label as PyTorch tensors\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'][0],\n",
    "            'attention_mask': inputs['attention_mask'][0],\n",
    "            # 'token_type_ids': inputs['token_type_ids'][0],\n",
    "            'label': torch.tensor(label, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "# datasetclass = CustomDataset(tokenizer, train)\n",
    "train_dataset = CustomDataset(tokenizer, train)\n",
    "val_dataset = CustomDataset(tokenizer, val)\n",
    "# test_dataset = CustomDataset(tokenizer, test)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 256\n",
    "train_dataloader = tqdm(DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=64))\n",
    "val_dataloader = tqdm(DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=64))\n",
    "# test_dataloader = tqdm(DataLoader(test_dataset, batch_size=batch_size, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        # self.Longformer = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "        # Freeze all layers except the top 1\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the parameters of the top 1 layers\n",
    "        for param in self.roberta.encoder.layer[-1:].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # self.xlnet.resize_token_embeddings(num_tokens)\n",
    "        # self.transformer_encoder = TransformerEncoder(TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads), num_layers=num_layers)\n",
    "        #self.transformer_decoder = TransformerDecoder(TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads), num_layers=num_layers)\n",
    "        #self.transformer = Transformer(nhead=16, num_encoder_layers=6, num_decoder_layers = 6)\n",
    "        self.decoder = nn.Linear(self.roberta.config.hidden_size, num_labels) \n",
    "        # self.fc1 = nn.Linear(num_tokens, 2)\n",
    "        # self.fc2 = nn.Linear(num_tokens, 2)\n",
    "        # self.fc3 = nn.Linear(num_tokens, 5)\n",
    "        # self.num_classes = num_classes\n",
    "        # self.classifiers = nn.ModuleList([nn.Linear(self.roberta.config.hidden_size, num_classes[i]) for i in range(len(num_classes))])\n",
    "        # self.classifiers = nn.ModuleList([nn.Linear(num_tokens, num_classes[i]) for i in range(len(num_classes))])\n",
    "        # self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):  # src = [bsz, seq_len]\n",
    "        long_output = self.roberta(input_ids=input_ids).pooler_output\n",
    "        # print(long_output.shape)\n",
    "        # roberta_outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # last_hidden_state = outputs.last_hidden_state # Shape: (batch_size, sequence_len, hidden_size)\n",
    "        # src_embedded = last_hidden_state\n",
    "        # src_embedded = self.roberta.embeddings(src) # Use RoBERTa model to embed source sequence output: [bsz, seq_len, features,i.e. hidden_dim] [20, 100, 768]\n",
    "        # print(\"shape of roberta embeddings:\", src_embedded.shape)\n",
    "        #tgt_embedded = self.roberta.embeddings(tgt) # Use RoBERTa model to embed target sequence\n",
    "        # src_embedded = src_embedded # output: [bsz, seq_len, features] \n",
    "        # src_embedded = torch.cat([t1,t2,t3, src_embedded],1)\n",
    "\n",
    "        # t1 = torch.cat(src_embedded.size(0) * [t1])\n",
    "        # t2 = torch.cat(src_embedded.size(0) * [t2])\n",
    "        # t3 = torch.cat(src_embedded.size(0) * [t3])\n",
    "        # t = torch.stack([t1,t2,t3], dim=1)\n",
    "        # task_embedded = torch.cat([t, src_embedded],1)  # output shape: [bsz, seq_len, features] [8, 203, 768]\n",
    "\n",
    "        # memory = self.transformer_encoder(src_embedded)  # output shape: [bsz, seq_len, features] [8, 203, 768]\n",
    "        # print(\"shape after transformer encoder layer:\", memory.shape)\n",
    "        #output = self.transformer_decoder(tgt_embedded, memory)\n",
    "        #print(\"shape after transformer decoder layer:\", output.shape)\n",
    "\n",
    "        output = self.decoder(long_output)  # output shape: [bsz, seq_len, vocab_size] [8, 203, 50k]\n",
    "        # print(\"shape after transformer decoder layer:\", output.shape, output.dtype)\n",
    "        # task1_output = self.fc1(output[:,0,:])\n",
    "        # task2_output = self.fc2(output[:,1,:])\n",
    "        # task3_output = self.fc3(output[:,2,:num_classes])\n",
    "        # ae_output = output[:,len(self.num_classes):,:]\n",
    "        # ae_output = output[:,:,:]\n",
    "        # print(\"shape after final linear layer:\", output.shape)\n",
    "        # task_logits = [classifier(pooled_output) for classifier in self.classifiers]\n",
    "        # task_logits = []\n",
    "\n",
    "        # pooled_outputs = [output[:,i,:] for i in range(len(self.num_classes))] # output shape : [bsz, 1, vocab_size]\n",
    "\n",
    "        # for classifier, pooled_output in zip(self.classifiers, pooled_outputs):\n",
    "        #     # pooled_output = self.tanh(pooled_output)\n",
    "        #     logits = classifier(pooled_output)\n",
    "        #     task_logits.append(logits)\n",
    "        \n",
    "        return output\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = 5\n",
    "\n",
    "model = TransformerModel(num_labels).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 5\n",
    "learning_rate = 2e-4\n",
    "class_weights = torch.tensor([12.0, 1.0, 1.0, 8.0, 11.0]).to(device)\n",
    "\n",
    "# Set optimizer and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(logit, targets):\n",
    "    \"\"\"\n",
    "    Calculate accuracy and macro F1 score for each class\n",
    "    \"\"\"\n",
    "    # pos = list(task_dict.keys()).index(task_name)\n",
    "    # mask = torch.arange(targets.shape[0]).to(device)\n",
    "    # task_idx = mask[targets[:,pos] != 99]\n",
    "    output = logit\n",
    "    true_label = targets\n",
    "    # print(\"shapes for label:\", output.shape, true_label.shape)\n",
    "    pred_label = torch.argmax(output, dim=1).flatten().tolist()\n",
    "    true_label = true_label.flatten().tolist()\n",
    "\n",
    "\n",
    "    return pred_label, true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_train_loss = []\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 1\n",
    "    start_time = time.time()\n",
    "    num_batches = len(train_dataset) // batch_size\n",
    "    for batch, i in enumerate(train_dataloader):\n",
    "        data, mask, targets = i.values()\n",
    "        data = data.to(device)\n",
    "        mask = mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # print(data.dtype)        \n",
    "        # print(data.shape)\n",
    "        # task_logits, ae_output = model(data)\n",
    "        output = model(data, mask)\n",
    "        # t1_out, t2_out, t3_out, auto_output = model(data, t1, t2, t3)\n",
    "        # loss = custom_loss(logits_task1, logits_task2, logits_task3, targets)\n",
    "        # print(\"shape:\", data.shape, targets.flatten().shape)\n",
    "        # print(\"outputtype:\", output.dtype, targets.flatten().dtype)\n",
    "        # print(output)\n",
    "        # targets = targets.float()\n",
    "        loss = criterion(output, targets.flatten())\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            # ppl = np.exp(cur_loss)\n",
    "            # print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "            #         f'lr {lr:02.7f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "            #         f'loss {cur_loss:5.5f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # if batch == 100:\n",
    "        #     break\n",
    "    current_train_loss.append(cur_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    # src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        for batch, i in enumerate(val_dataloader):\n",
    "            data, mask, targets = i.values()\n",
    "            data = data.to(device)\n",
    "            mask = mask.to(device)\n",
    "            targets = targets.to(device)\n",
    "            seq_len = data.size(1)\n",
    "            # logits_task1, logits_task2, logits_task3, ae_output = model(data, mask)\n",
    "            # task_logits, ae_output = model(data)\n",
    "            # task_logits, ae_output = model(data, mask)\n",
    "            output = model(data, mask)\n",
    "            # t1_out, t2_out, t3_out, auto_output = model(data, t1, t2, t3)\n",
    "            # loss = custom_loss(logits_task1, logits_task2, logits_task3, targets)\n",
    "            # loss = custom_loss(logits_task1, logits_task2, logits_task3, ae_output, data, targets)\n",
    "            loss = criterion(output, targets.flatten())\n",
    "\n",
    "            total_loss += seq_len * loss.item()\n",
    "\n",
    "            #get the labels for classification report\n",
    "            pred_label, true_label = get_labels(output, targets)\n",
    "            predictions.extend(pred_label)\n",
    "            true_labels.extend(true_label)\n",
    "            # if batch == 100:\n",
    "            #     break\n",
    "\n",
    "    # Compute overall classification report\n",
    "    logging.info(f\"\\n Scores:\")\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    logging.info(f\"\\n Accuracy:{acc}\")\n",
    "    logging.info(f\"\\n {classification_report(true_labels, predictions)}\")\n",
    "    return total_loss / (len(val_dataset) - 1), acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 19:57:09,815 - INFO - #########################################################################################\n",
      "2023-07-13 19:57:09,818 - INFO - \n",
      " DESCRIPTION-> \n",
      " logic: roberta(finetune last layer) + linear_layer + loss_reweighting (epochs=50), model: roberta-base, lr:0.0002, max_seq_length: 500\n",
      "2023-07-13 19:57:09,820 - INFO - #########################################################################################\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"#\"* 89)\n",
    "logging.info(f\"\\n DESCRIPTION-> \\n logic: roberta(finetune last layer) + linear_layer + loss_reweighting (epochs=50), model: {tokenizer.name_or_path}, lr:{learning_rate}, max_seq_length: {max_length}\")\n",
    "logging.info('#' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2134/2134 [1:56:05<00:00,  3.26s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [2:02:45<00:00, 51.87s/it]\n",
      "2023-07-13 21:59:50,945 - INFO - \n",
      " Scores:\n",
      "2023-07-13 21:59:50,958 - INFO - \n",
      " Accuracy:0.604462586535014\n",
      "2023-07-13 21:59:51,002 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.40      0.24      1158\n",
      "           1       0.76      0.62      0.68     16938\n",
      "           2       0.74      0.66      0.70     14848\n",
      "           3       0.22      0.38      0.28      1713\n",
      "           4       0.12      0.25      0.16      1600\n",
      "\n",
      "    accuracy                           0.60     36257\n",
      "   macro avg       0.40      0.46      0.41     36257\n",
      "weighted avg       0.68      0.60      0.63     36257\n",
      "\n",
      "2023-07-13 21:59:51,003 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-13 21:59:51,004 - INFO - | end of epoch   1 | time: 7361.17s | valid loss 2.11808\n",
      "2023-07-13 21:59:51,005 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 00:02:29,281 - INFO - \n",
      " Scores:\n",
      "2023-07-14 00:02:29,294 - INFO - \n",
      " Accuracy:0.5383236340568718\n",
      "2023-07-14 00:02:29,358 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.57      0.28      1158\n",
      "           1       0.77      0.53      0.63     16938\n",
      "           2       0.74      0.58      0.65     14848\n",
      "           3       0.23      0.38      0.29      1713\n",
      "           4       0.09      0.35      0.14      1600\n",
      "\n",
      "    accuracy                           0.54     36257\n",
      "   macro avg       0.40      0.48      0.40     36257\n",
      "weighted avg       0.68      0.54      0.59     36257\n",
      "\n",
      "2023-07-14 00:02:29,360 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 00:02:29,361 - INFO - | end of epoch   2 | time: 7357.73s | valid loss 2.29923\n",
      "2023-07-14 00:02:29,362 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 02:05:21,890 - INFO - \n",
      " Scores:\n",
      "2023-07-14 02:05:21,902 - INFO - \n",
      " Accuracy:0.5991946382767466\n",
      "2023-07-14 02:05:21,948 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.43      0.32      1158\n",
      "           1       0.77      0.60      0.67     16938\n",
      "           2       0.71      0.67      0.69     14848\n",
      "           3       0.24      0.32      0.27      1713\n",
      "           4       0.12      0.38      0.18      1600\n",
      "\n",
      "    accuracy                           0.60     36257\n",
      "   macro avg       0.42      0.48      0.43     36257\n",
      "weighted avg       0.68      0.60      0.63     36257\n",
      "\n",
      "2023-07-14 02:05:21,949 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 02:05:21,950 - INFO - | end of epoch   3 | time: 7372.59s | valid loss 2.17323\n",
      "2023-07-14 02:05:21,951 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 04:08:08,433 - INFO - \n",
      " Scores:\n",
      "2023-07-14 04:08:08,446 - INFO - \n",
      " Accuracy:0.6086548804368812\n",
      "2023-07-14 04:08:08,491 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.49      0.31      1158\n",
      "           1       0.77      0.60      0.67     16938\n",
      "           2       0.73      0.68      0.71     14848\n",
      "           3       0.23      0.37      0.28      1713\n",
      "           4       0.15      0.38      0.21      1600\n",
      "\n",
      "    accuracy                           0.61     36257\n",
      "   macro avg       0.42      0.50      0.44     36257\n",
      "weighted avg       0.69      0.61      0.64     36257\n",
      "\n",
      "2023-07-14 04:08:08,493 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 04:08:08,494 - INFO - | end of epoch   4 | time: 7366.54s | valid loss 2.22225\n",
      "2023-07-14 04:08:08,495 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 05:31:21,154 - INFO - \n",
      " Scores:\n",
      "2023-07-14 05:31:21,169 - INFO - \n",
      " Accuracy:0.6221970929751496\n",
      "2023-07-14 05:31:21,211 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.47      0.30      1158\n",
      "           1       0.74      0.67      0.70     16938\n",
      "           2       0.75      0.65      0.70     14848\n",
      "           3       0.24      0.34      0.28      1713\n",
      "           4       0.16      0.32      0.21      1600\n",
      "\n",
      "    accuracy                           0.62     36257\n",
      "   macro avg       0.42      0.49      0.44     36257\n",
      "weighted avg       0.68      0.62      0.65     36257\n",
      "\n",
      "2023-07-14 05:31:21,213 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 05:31:21,213 - INFO - | end of epoch   5 | time: 4979.84s | valid loss 2.21426\n",
      "2023-07-14 05:31:21,214 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 06:47:49,199 - INFO - \n",
      " Scores:\n",
      "2023-07-14 06:47:49,213 - INFO - \n",
      " Accuracy:0.6115784538158149\n",
      "2023-07-14 06:47:49,252 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.40      0.31      1158\n",
      "           1       0.76      0.62      0.69     16938\n",
      "           2       0.72      0.66      0.69     14848\n",
      "           3       0.23      0.41      0.30      1713\n",
      "           4       0.15      0.37      0.22      1600\n",
      "\n",
      "    accuracy                           0.61     36257\n",
      "   macro avg       0.42      0.49      0.44     36257\n",
      "weighted avg       0.68      0.61      0.64     36257\n",
      "\n",
      "2023-07-14 06:47:49,254 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 06:47:49,255 - INFO - | end of epoch   6 | time: 4578.98s | valid loss 2.30411\n",
      "2023-07-14 06:47:49,256 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 08:04:07,409 - INFO - \n",
      " Scores:\n",
      "2023-07-14 08:04:07,423 - INFO - \n",
      " Accuracy:0.6199078798576827\n",
      "2023-07-14 08:04:07,470 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.45      0.30      1158\n",
      "           1       0.74      0.66      0.70     16938\n",
      "           2       0.74      0.65      0.69     14848\n",
      "           3       0.23      0.37      0.28      1713\n",
      "           4       0.18      0.34      0.23      1600\n",
      "\n",
      "    accuracy                           0.62     36257\n",
      "   macro avg       0.42      0.49      0.44     36257\n",
      "weighted avg       0.68      0.62      0.64     36257\n",
      "\n",
      "2023-07-14 08:04:07,471 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 08:04:07,472 - INFO - | end of epoch   7 | time: 4578.21s | valid loss 2.46155\n",
      "2023-07-14 08:04:07,473 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 09:20:26,581 - INFO - \n",
      " Scores:\n",
      "2023-07-14 09:20:26,594 - INFO - \n",
      " Accuracy:0.6373941583694184\n",
      "2023-07-14 09:20:26,651 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.37      0.30      1158\n",
      "           1       0.78      0.63      0.70     16938\n",
      "           2       0.71      0.73      0.72     14848\n",
      "           3       0.26      0.40      0.31      1713\n",
      "           4       0.19      0.35      0.25      1600\n",
      "\n",
      "    accuracy                           0.64     36257\n",
      "   macro avg       0.44      0.49      0.45     36257\n",
      "weighted avg       0.68      0.64      0.65     36257\n",
      "\n",
      "2023-07-14 09:20:26,653 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 09:20:26,654 - INFO - | end of epoch   8 | time: 4579.18s | valid loss 2.47869\n",
      "2023-07-14 09:20:26,654 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 10:36:50,375 - INFO - \n",
      " Scores:\n",
      "2023-07-14 10:36:50,388 - INFO - \n",
      " Accuracy:0.6225556444272831\n",
      "2023-07-14 10:36:50,434 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.33      0.30      1158\n",
      "           1       0.78      0.60      0.68     16938\n",
      "           2       0.70      0.72      0.71     14848\n",
      "           3       0.24      0.40      0.30      1713\n",
      "           4       0.16      0.35      0.22      1600\n",
      "\n",
      "    accuracy                           0.62     36257\n",
      "   macro avg       0.43      0.48      0.44     36257\n",
      "weighted avg       0.68      0.62      0.64     36257\n",
      "\n",
      "2023-07-14 10:36:50,436 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 10:36:50,437 - INFO - | end of epoch   9 | time: 4577.38s | valid loss 2.60589\n",
      "2023-07-14 10:36:50,438 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 12:21:34,562 - INFO - \n",
      " Scores:\n",
      "2023-07-14 12:21:34,573 - INFO - \n",
      " Accuracy:0.652177510549687\n",
      "2023-07-14 12:21:34,615 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.35      0.29      1158\n",
      "           1       0.75      0.68      0.72     16938\n",
      "           2       0.72      0.71      0.72     14848\n",
      "           3       0.30      0.31      0.31      1713\n",
      "           4       0.21      0.36      0.26      1600\n",
      "\n",
      "    accuracy                           0.65     36257\n",
      "   macro avg       0.44      0.48      0.46     36257\n",
      "weighted avg       0.68      0.65      0.66     36257\n",
      "\n",
      "2023-07-14 12:21:34,617 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 12:21:34,618 - INFO - | end of epoch  10 | time: 6284.18s | valid loss 2.58427\n",
      "2023-07-14 12:21:34,619 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 14:24:13,422 - INFO - \n",
      " Scores:\n",
      "2023-07-14 14:24:13,433 - INFO - \n",
      " Accuracy:0.6519844443831536\n",
      "2023-07-14 14:24:13,474 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.36      0.32      1158\n",
      "           1       0.76      0.67      0.71     16938\n",
      "           2       0.71      0.72      0.72     14848\n",
      "           3       0.30      0.34      0.32      1713\n",
      "           4       0.21      0.36      0.26      1600\n",
      "\n",
      "    accuracy                           0.65     36257\n",
      "   macro avg       0.45      0.49      0.47     36257\n",
      "weighted avg       0.68      0.65      0.66     36257\n",
      "\n",
      "2023-07-14 14:24:13,476 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 14:24:13,476 - INFO - | end of epoch  11 | time: 7353.02s | valid loss 2.66539\n",
      "2023-07-14 14:24:13,477 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 16:26:43,861 - INFO - \n",
      " Scores:\n",
      "2023-07-14 16:26:43,881 - INFO - \n",
      " Accuracy:0.6574730396888877\n",
      "2023-07-14 16:26:43,925 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.34      0.31      1158\n",
      "           1       0.76      0.67      0.72     16938\n",
      "           2       0.71      0.73      0.72     14848\n",
      "           3       0.26      0.36      0.30      1713\n",
      "           4       0.24      0.31      0.27      1600\n",
      "\n",
      "    accuracy                           0.66     36257\n",
      "   macro avg       0.45      0.48      0.46     36257\n",
      "weighted avg       0.68      0.66      0.67     36257\n",
      "\n",
      "2023-07-14 16:26:43,927 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 16:26:43,928 - INFO - | end of epoch  12 | time: 7350.45s | valid loss 2.66600\n",
      "2023-07-14 16:26:43,929 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 18:29:29,018 - INFO - \n",
      " Scores:\n",
      "2023-07-14 18:29:29,032 - INFO - \n",
      " Accuracy:0.6544943045480872\n",
      "2023-07-14 18:29:29,073 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.35      0.31      1158\n",
      "           1       0.76      0.69      0.72     16938\n",
      "           2       0.71      0.72      0.71     14848\n",
      "           3       0.27      0.33      0.30      1713\n",
      "           4       0.20      0.30      0.24      1600\n",
      "\n",
      "    accuracy                           0.65     36257\n",
      "   macro avg       0.44      0.48      0.46     36257\n",
      "weighted avg       0.68      0.65      0.66     36257\n",
      "\n",
      "2023-07-14 18:29:29,074 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 18:29:29,075 - INFO - | end of epoch  13 | time: 7359.28s | valid loss 2.79075\n",
      "2023-07-14 18:29:29,076 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 20:32:02,120 - INFO - \n",
      " Scores:\n",
      "2023-07-14 20:32:02,138 - INFO - \n",
      " Accuracy:0.6707394434178228\n",
      "2023-07-14 20:32:02,176 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.28      0.28      1158\n",
      "           1       0.75      0.71      0.73     16938\n",
      "           2       0.71      0.73      0.72     14848\n",
      "           3       0.30      0.34      0.32      1713\n",
      "           4       0.26      0.33      0.29      1600\n",
      "\n",
      "    accuracy                           0.67     36257\n",
      "   macro avg       0.46      0.48      0.47     36257\n",
      "weighted avg       0.68      0.67      0.67     36257\n",
      "\n",
      "2023-07-14 20:32:02,178 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 20:32:02,178 - INFO - | end of epoch  14 | time: 7353.10s | valid loss 2.76308\n",
      "2023-07-14 20:32:02,179 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 22:34:45,296 - INFO - \n",
      " Scores:\n",
      "2023-07-14 22:34:45,309 - INFO - \n",
      " Accuracy:0.6578040102600877\n",
      "2023-07-14 22:34:45,353 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.32      0.29      1158\n",
      "           1       0.75      0.68      0.71     16938\n",
      "           2       0.71      0.73      0.72     14848\n",
      "           3       0.32      0.36      0.34      1713\n",
      "           4       0.21      0.30      0.25      1600\n",
      "\n",
      "    accuracy                           0.66     36257\n",
      "   macro avg       0.45      0.48      0.46     36257\n",
      "weighted avg       0.67      0.66      0.66     36257\n",
      "\n",
      "2023-07-14 22:34:45,355 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-14 22:34:45,356 - INFO - | end of epoch  15 | time: 7356.84s | valid loss 2.94734\n",
      "2023-07-14 22:34:45,356 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-15 00:37:25,352 - INFO - \n",
      " Scores:\n",
      "2023-07-15 00:37:25,365 - INFO - \n",
      " Accuracy:0.6644510025650219\n",
      "2023-07-15 00:37:25,409 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.34      0.30      1158\n",
      "           1       0.76      0.68      0.72     16938\n",
      "           2       0.71      0.74      0.72     14848\n",
      "           3       0.32      0.35      0.33      1713\n",
      "           4       0.24      0.32      0.28      1600\n",
      "\n",
      "    accuracy                           0.66     36257\n",
      "   macro avg       0.46      0.49      0.47     36257\n",
      "weighted avg       0.68      0.66      0.67     36257\n",
      "\n",
      "2023-07-15 00:37:25,412 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-15 00:37:25,413 - INFO - | end of epoch  16 | time: 7360.05s | valid loss 2.99269\n",
      "2023-07-15 00:37:25,414 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-15 02:40:10,474 - INFO - \n",
      " Scores:\n",
      "2023-07-15 02:40:10,488 - INFO - \n",
      " Accuracy:0.6544391427862206\n",
      "2023-07-15 02:40:10,530 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.36      0.29      1158\n",
      "           1       0.75      0.68      0.71     16938\n",
      "           2       0.71      0.72      0.72     14848\n",
      "           3       0.27      0.33      0.30      1713\n",
      "           4       0.24      0.30      0.26      1600\n",
      "\n",
      "    accuracy                           0.65     36257\n",
      "   macro avg       0.44      0.48      0.46     36257\n",
      "weighted avg       0.67      0.65      0.66     36257\n",
      "\n",
      "2023-07-15 02:40:10,531 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-15 02:40:10,532 - INFO - | end of epoch  17 | time: 7365.12s | valid loss 3.17700\n",
      "2023-07-15 02:40:10,532 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-15 04:42:49,051 - INFO - \n",
      " Scores:\n",
      "2023-07-15 04:42:49,064 - INFO - \n",
      " Accuracy:0.6636787378988885\n",
      "2023-07-15 04:42:49,101 - INFO - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.31      0.31      1158\n",
      "           1       0.74      0.71      0.72     16938\n",
      "           2       0.72      0.71      0.72     14848\n",
      "           3       0.31      0.34      0.32      1713\n",
      "           4       0.23      0.32      0.26      1600\n",
      "\n",
      "    accuracy                           0.66     36257\n",
      "   macro avg       0.46      0.48      0.47     36257\n",
      "weighted avg       0.67      0.66      0.67     36257\n",
      "\n",
      "2023-07-15 04:42:49,102 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-15 04:42:49,103 - INFO - | end of epoch  18 | time: 7358.57s | valid loss 3.14256\n",
      "2023-07-15 04:42:49,104 - INFO - -----------------------------------------------------------------------------------------\n",
      "2023-07-15 04:42:49,105 - INFO - Early stopped training at epoch 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "current_val_loss = []   # for plotting graph of val_loss\n",
    "epochs = 50\n",
    "early_stop_thresh = 3\n",
    "\n",
    "tempdir = '/data1/debajyoti/colie/.temp/'\n",
    "best_model_params_path = os.path.join(tempdir, f\"best_model_params_{time.asctime().replace(' ','_')}.pt\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss, accuracy = evaluate(model)\n",
    "    current_val_loss.append(val_loss)\n",
    "    # val_ppl = np.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logging.info('-' * 89)\n",
    "    logging.info(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "        f'valid loss {val_loss:5.5f}')\n",
    "    logging.info('-' * 89)\n",
    "\n",
    "    if accuracy > best_val_acc:\n",
    "        best_val_acc = accuracy\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        logging.info(\"Early stopped training at epoch %d\" % epoch)\n",
    "        break  # terminate the training loop\n",
    "\n",
    "    scheduler.step()\n",
    "model.load_state_dict(torch.load(best_model_params_path)) # load best model states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Alas, poor girl!\" said I, \"I fear that her ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to divide her attention between the said garco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>visitor's disposition to gallantry. However, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>says Juvenal, \"'Mors sola fatetur Quantula sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>him out in that back passage; the outer door i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143025</th>\n",
       "      <td>be hard for anyone to do anything dignified, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143026</th>\n",
       "      <td>Wilson, and could not bring himself to think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143027</th>\n",
       "      <td>give them a chance, the same as everybody else...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143028</th>\n",
       "      <td>political convention in which he declared that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143029</th>\n",
       "      <td>Jimmie doesn't know anything about the Russian...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143030 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "0       \"Alas, poor girl!\" said I, \"I fear that her ha...\n",
       "1       to divide her attention between the said garco...\n",
       "2       visitor's disposition to gallantry. However, s...\n",
       "3       says Juvenal, \"'Mors sola fatetur Quantula sin...\n",
       "4       him out in that back passage; the outer door i...\n",
       "...                                                   ...\n",
       "143025  be hard for anyone to do anything dignified, w...\n",
       "143026  Wilson, and could not bring himself to think t...\n",
       "143027  give them a chance, the same as everybody else...\n",
       "143028  political convention in which he declared that...\n",
       "143029  Jimmie doesn't know anything about the Russian...\n",
       "\n",
       "[143030 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "class CustomDataset_test(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        # Initialize the tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the text and label from the dataframe\n",
    "        text = self.df.iloc[index]['text']\n",
    "        # label = self.df.iloc[index]['label']\n",
    "\n",
    "        # Tokenize the text and convert it to input IDs\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=False,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "\n",
    "        # Return the input IDs and label as PyTorch tensors\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'][0],\n",
    "            'attention_mask': inputs['attention_mask'][0],\n",
    "            # 'token_type_ids': inputs['token_type_ids'][0],\n",
    "            # 'label': torch.tensor(label, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "test_dataset = CustomDataset_test(tokenizer, test)\n",
    "\n",
    "# DataLoader\n",
    "test_dataloader = tqdm(DataLoader(test_dataset, batch_size=batch_size, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 559/559 [33:50<00:00,  3.63s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "# Evaluate model on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_dataloader):\n",
    "        data, mask = batch.values()\n",
    "        data = data.to(device)\n",
    "        mask = mask.to(device)\n",
    "        # targets = targets.to(device)\n",
    "        seq_len = data.size(1)\n",
    "        # logits_task1, logits_task2, logits_task3, ae_output = model(data, mask)\n",
    "        # task_logits, ae_output = model(data)\n",
    "        # task_logits, ae_output = model(data, mask)\n",
    "        output = model(data, mask)\n",
    "        #get the labels for classification report\n",
    "        pred_label = torch.argmax(output, dim=1).flatten().tolist()\n",
    "        predictions.extend(pred_label)\n",
    "        \n",
    "        # if batch == 400:\n",
    "        #     break\n",
    "\n",
    "# # Compute overall classification report\n",
    "# logging.info(f\"\\n Scores:\")\n",
    "# logging.info(f\"\\n {classification_report(true_labels, predictions)}\")\n",
    "# return total_loss / (len(val_dataset) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[\"Epoch\"] = predictions\n",
    "# test_labels.to_csv('file_name.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOOK_id</th>\n",
       "      <th>Epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7616_1.txt</td>\n",
       "      <td>Viktorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7616_2.txt</td>\n",
       "      <td>Viktorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7616_3.txt</td>\n",
       "      <td>Viktorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7616_4.txt</td>\n",
       "      <td>Viktorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7616_5.txt</td>\n",
       "      <td>Viktorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143025</th>\n",
       "      <td>5677_92.txt</td>\n",
       "      <td>Viktorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143026</th>\n",
       "      <td>5677_93.txt</td>\n",
       "      <td>Modernism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143027</th>\n",
       "      <td>5677_94.txt</td>\n",
       "      <td>Modernism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143028</th>\n",
       "      <td>5677_95.txt</td>\n",
       "      <td>Modernism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143029</th>\n",
       "      <td>5677_96.txt</td>\n",
       "      <td>Modernism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143030 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BOOK_id      Epoch\n",
       "0        7616_1.txt  Viktorian\n",
       "1        7616_2.txt  Viktorian\n",
       "2        7616_3.txt  Viktorian\n",
       "3        7616_4.txt  Viktorian\n",
       "4        7616_5.txt  Viktorian\n",
       "...             ...        ...\n",
       "143025  5677_92.txt  Viktorian\n",
       "143026  5677_93.txt  Modernism\n",
       "143027  5677_94.txt  Modernism\n",
       "143028  5677_95.txt  Modernism\n",
       "143029  5677_96.txt  Modernism\n",
       "\n",
       "[143030 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dic = {0:'Romanticism',\n",
    "            1:'Viktorian',\n",
    "            2:'Modernism',\n",
    "            3:'PostModernism',\n",
    "            4:'OurDays'}\n",
    "test_labels['Epoch'] = test_labels['Epoch'].map(label_dic)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.to_csv('/data1/debajyoti/colie/submission/submission_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature extractor using trigrams\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "\n",
    "# Transform the data into trigram feature vectors\n",
    "X_trigrams = trigram_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature extractor using bag of words\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the data into bag of words feature vectors\n",
    "X_bow = bow_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the trigram features and bag of words features\n",
    "X_features = hstack([X_trigrams, X_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection using chi2 #2KFH\n",
    "k = 400 # Select the top k features \n",
    "feature_selector = SelectKBest(chi2, k=k)\n",
    "X_selected = feature_selector.fit_transform(X_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation and print the classification report\n",
    "cv_results = cross_val_score(nb_classifier, X_selected, y, cv=10, scoring='accuracy')\n",
    "classification_report_cv = classification_report(y, cross_val_predict(nb_classifier, X_selected, y, cv=10))\n",
    "mean_accuracy = cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rifle; Ivan's was a double-barrelled shot-gun ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upon the track of the bear. After following it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to pull him out with their hands--even had the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a slight sparkle of scientific conceit, \"this ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bears with a white ring round their necks? Yes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546205</th>\n",
       "      <td>the manner described in the text, might lay cl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546206</th>\n",
       "      <td>surnamed, answered, ÃƒÂ¢Ã‚Â€Ã‚Â˜Na, na, there are na...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546207</th>\n",
       "      <td>that of Themis. My informant was Alexander Kei...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546208</th>\n",
       "      <td>a heavy blow. cloyed a dud, stolen a rag. coll...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546209</th>\n",
       "      <td>shanks, legs. shealing, sheiling, a shed, a hu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546210 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       rifle; Ivan's was a double-barrelled shot-gun ...      1\n",
       "1       upon the track of the bear. After following it...      1\n",
       "2       to pull him out with their hands--even had the...      1\n",
       "3       a slight sparkle of scientific conceit, \"this ...      1\n",
       "4       bears with a white ring round their necks? Yes...      1\n",
       "...                                                   ...    ...\n",
       "546205  the manner described in the text, might lay cl...      2\n",
       "546206  surnamed, answered, ÃƒÂ¢Ã‚Â€Ã‚Â˜Na, na, there are na...      2\n",
       "546207  that of Themis. My informant was Alexander Kei...      2\n",
       "546208  a heavy blow. cloyed a dud, stolen a rag. coll...      2\n",
       "546209  shanks, legs. shealing, sheiling, a shed, a hu...      2\n",
       "\n",
       "[546210 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([train, val], ignore_index=True, axis=0)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have separate dataframes for train, val, and test\n",
    "train_data = train_df['text']\n",
    "train_labels = train_df['label']\n",
    "\n",
    "# val_data = val['text']\n",
    "# val_labels = val['label']\n",
    "\n",
    "test_data = test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "# val_vectors = vectorizer.transform(val_data)\n",
    "test_vectors = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature extractor using trigrams\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "\n",
    "# Transform the data into trigram feature vectors\n",
    "train_vectors_trigrams = trigram_vectorizer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the trigram features and bag of words features\n",
    "X_features = hstack([train_vectors, train_vectors_trigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection using chi2 #2KFH\n",
    "k = 2400 # Select the top k features \n",
    "feature_selector = SelectKBest(chi2, k=k)\n",
    "X_selected = feature_selector.fit_transform(X_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_selected, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5512879305459965"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform cross-validation and print the classification report\n",
    "cv_results = cross_val_score(classifier, X_selected, train_labels, cv=10, scoring='accuracy')\n",
    "classification_report_cv = classification_report(train_labels, cross_val_predict(classifier, X_selected, train_labels, cv=10))\n",
    "mean_accuracy = cv_results.mean()\n",
    "mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.67      0.27     21319\n",
      "           1       0.66      0.53      0.59    257303\n",
      "           2       0.70      0.61      0.65    252936\n",
      "           3       0.21      0.41      0.27     23852\n",
      "           4       0.23      0.17      0.20     27057\n",
      "\n",
      "    accuracy                           0.55    582467\n",
      "   macro avg       0.39      0.48      0.40    582467\n",
      "weighted avg       0.62      0.55      0.57    582467\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = classifier.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[\"Epoch\"] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOOK_id</th>\n",
       "      <th>Epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7616_1.txt</td>\n",
       "      <td>Viktorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7616_2.txt</td>\n",
       "      <td>Viktorian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7616_3.txt</td>\n",
       "      <td>Romanticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7616_4.txt</td>\n",
       "      <td>Romanticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7616_5.txt</td>\n",
       "      <td>Romanticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143025</th>\n",
       "      <td>5677_92.txt</td>\n",
       "      <td>Modernism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143026</th>\n",
       "      <td>5677_93.txt</td>\n",
       "      <td>Modernism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143027</th>\n",
       "      <td>5677_94.txt</td>\n",
       "      <td>Modernism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143028</th>\n",
       "      <td>5677_95.txt</td>\n",
       "      <td>Modernism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143029</th>\n",
       "      <td>5677_96.txt</td>\n",
       "      <td>Modernism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143030 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BOOK_id        Epoch\n",
       "0        7616_1.txt    Viktorian\n",
       "1        7616_2.txt    Viktorian\n",
       "2        7616_3.txt  Romanticism\n",
       "3        7616_4.txt  Romanticism\n",
       "4        7616_5.txt  Romanticism\n",
       "...             ...          ...\n",
       "143025  5677_92.txt    Modernism\n",
       "143026  5677_93.txt    Modernism\n",
       "143027  5677_94.txt    Modernism\n",
       "143028  5677_95.txt    Modernism\n",
       "143029  5677_96.txt    Modernism\n",
       "\n",
       "[143030 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dic = {0:'Romanticism',\n",
    "            1:'Viktorian',\n",
    "            2:'Modernism',\n",
    "            3:'PostModernism',\n",
    "            4:'OurDays'}\n",
    "test_labels['Epoch'] = test_labels['Epoch'].map(label_dic)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.to_csv('/data1/debajyoti/colie/submission/submission_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
